<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[读书笔记：从0到1:开启商业与未来的秘密]]></title>
    <url>%2FZeroToOne%2F</url>
    <content type="text"><![CDATA[My notes of book Zero to One: Notes on Startups, or How to Build the Future 一个问题在什么重要问题上你与他人看法不同？ 反主流创新不是抵制潮流，而是在潮流中不丢弃自己的独立思考 垄断垄断者的谎言隐瞒垄断。将自己置于另一个更大的市场，躲开注意力。 非垄断者的谎言把自己市场定义为各种小市场的交集来夸大自己的独特性。 垄断对于业内的好处更大的自主权去关心自己的员工、产品、影响力。有机会严守道德，着眼于长期利益（垄断利益）。 竞争竞争意味着大家都减少利润 如何一步一步走向垄断 “成功不是中彩票”，成功不能靠运气，而是要有清晰长远的规划 参考资料 《从0到1 开启商业与未来的秘密》札记 《从0到1：开启商业与未来的秘密》读书笔记]]></content>
      <categories>
        <category>Notes</category>
        <category>Business</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Startups</tag>
        <tag>Business</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记：祖鲁法则:如何选择成长股]]></title>
    <url>%2Fzulu%2F</url>
    <content type="text"><![CDATA[My notes of book Beyond the Zulu Principle 概述投资三大方法 成长股投资：选择有良好成长前景的股票，在其每股收益EPS逐年增长的过程中，从复合效应获益 资产状况投资：当公司股票跌破其潜在价值或其他极端情况下买入 技术分析法：仅仅考虑价格变动，变量，图标进行操作 选择比时机更重要 成长股投资时，更重要的是投资的选择，而非时机 投资于好的行业 不必花费过多时间担忧市场走势。一旦对自己投资有自信，对市场的担忧也会少一些 卓悦成长股的特点 正确的行业 有竞争优势的公司 良好的管理能力 每股收益持续增长 选择成长股7大标准第一大标准：市盈率相对盈利增长比率（PEG） PEG&lt;1.0才值得投资 不适用于周期性股票 固定对比周期：使用12个月的滚动周期进行对比 注意税率的影响：税率上升可能掩盖真实的增长 市盈率（PE）的影响：较低PE能使PEG更好的发挥作用，且在熊市更安全 注意高增长情况：50%算拥有极高的增长速度。由一家公司市盈率状态的改变所引起的股价大幅上扬，很有可能发生逆转。一次相对微小的后退，或许会轻易引起魔域天文数字般的声音蹦溃 第二大标准：现金流现金流能作用： 未来的股利是否安全 流动性和负债的大致变动趋势 一家公司是否过度扩张营运 未来的扩张计划及资本支出计划能否从公司内部获得资金支持 第三大标准：相对强度 相对强度作用：在实践中，表现最好的股票常常是那些已经战胜市场，并开始以胜利者姿态行事的公司。 在熊市中，相对强弱的筛选标准不会这么有效 平均线：突破移动平均线，通常是一个可靠的看涨信号。相反，股票价格跌破移动平均线，通常是一个看跌的信号 第四大标准：管理能力 成熟公司的每股收益增长记录，是判断管理层能力的最好方法之一 对于相对不成熟的公司，每股收益增长率非常重要（在记录足够长的前提下） 董事们的股份交易信息非常有用。董事买入/卖出要看其具体的买入比例，而非绝对数值。 第五大标准:竞争优势 极高的利润率会招致竞争 极低的利润率会增加投资的风险 考察一家公司的利润率时，应当将它与其他同类公司的利润率，以及行业的平均水平进行比较 利润率的趋势至关重要。利润率的下跌有可能是第一个及唯一的一个警报信号，它表明一家公司正在丧失其竞争优势 第六大标准:财务状况 每股现金流仅仅是衡量一家公司财力的一个指标，资产负债率的水平，现金余额及一般的流动性，也是反映其困难时期生存能力的极重要的指标 资产负债率是公认的可以迅速反映一家公司借款程度的指标 第七大标准:每股收益 应当密切关注销售同比，假如这些数字开始下降，该公司可能遭遇到过度的竞争 避免投资于那些拥有可克隆的经营模式，但是没有盈利的公司，以及那些预期的市盈率超过20倍的公司 总结：评价成长股的标准在寻找要投资的股票时，从所有的上市股票开始，应用一系列的筛选标准。接着仔细研究通过这些筛选标准的股票，并试图识别他们正在拥有的优势，因为这正是他们拥有如此出色财务特征的原因。较高的运用资本回报率，以及良好的营运利润率，通常是一家公司拥有竞争优势的支持性证据。 强制性标准 拥有市盈率相对盈利增长比率（这至少意味着该公司拥有四年的增长模式，即使需要将未来的前景与过去的记录结合起来，才能够达到最低的增长年限要求） 不超过20倍的预期市盈率 强劲的现金流（每股收益年复一年地转变成现金，并且想看到它转化为巨大的现金余额，或者他至少能迅速降低资产负债率） 低资产负债率，或有现金结余 高相对强度 竞争优势 董事们的交易（假如一些董事正在积极减少股份，我无疑会卖出这支股票。董事们购买自己公司的股票是非常令人满意的情形，但是它不是必要的因素，不存在董事们的重大卖出则是强制性的要求） 可选择使用的标准 加速增长的每股收益 董事们买入自己公司的股票 市值 股利收益率 （支持股利的公司更可取，大多数的公司确实支付股利，而那些不支付股利的公司，则应当谨慎对待。） 加分标准 低股价与销售额比率 某样新东西 低股价与研发支出比率 合理的资产状况 投资管理投资策略 将相当大数量的资金投入自认为有所了解，并且自己能彻底相信公司管理的那些企业上面。 并非依据市场价格来判断自己所持有的股票，而是根据他们的经营业绩来加以判断。 通过若干股票来分散风险非常重要，但避免过量投资，投资保持在20组以下。 每只股票占比不超过15%。 投资总量不能让你晚上睡不着。 投资习惯 为每一份投资建档 确保每周至少投入一小时左右的时间，能以一种战略方式思考你的投资 何时卖出 长期来看，忽略市场变化，永久持有股票 资金需要用于投资更优组合 市场高估股票 利润停滞不前 有主要竞争者进入，开始价格战 丧失某一主要的业务来源 熊市的征兆 估波指标 现金是非常不受欢迎的一项资产 平均的股利收益率将会处于历史上极低的水平 利率通常即将上升 投资顾问对市场一致看涨，并且投资者普遍感到乐观 新股大量发行，并且其质量越来越差 董事买入数对董事卖出数的比率已经下跌到历史的低水平 股票将不能对好的业绩有所反应 市场将是鸡尾酒会和宴会上人人谈论的话题 当市场中超过75%的股票价格一直高于其长期的平均水平，假如接下来满足条件的股票数目下降到75%以下，这通常是一个熊市即将到来的技术信号 广义的货币供应量正在紧缩 市场领导力量将会发生重大改变，在牛市的顶峰附近，周期性股票通常表现较好 科技股 典型的科技成长公司，常常会出现一次“阵痛”。因此投资一家公司之前，等待它出现两次盈利警告是值得的（第一次不太可靠，此时管理层往往过于乐观） 核实研发支出成本。当心将研发支出成本化的公司 周期性股票 利率下降的第二年或者随后的年份，比下降的第一年更好 货币上涨 利率下降的最后一年是最佳时机 不要盲目，咨询业内人士，了解行业信息 小心低市盈率倍数 参考资料《祖鲁法则》读书笔记]]></content>
      <categories>
        <category>Notes</category>
        <category>Investment</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Investment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes of cs188 Artificial Intelligence]]></title>
    <url>%2Fcs188%2F</url>
    <content type="text"><![CDATA[My notes of UC Berkeley online course Intro to Artificial Intelligence SearchTwo type of searchPlanning: sequences of actions The path to the goal is the important thing Paths have various costs, depths Heuristics give problem-specific guidance Identification: assignments to variables The goal itself is important, not the path All paths at the same depth (for some formulations) CSPs are specialized for identification problems Uninformed and Informed Search (Planning) All search algorithms are the same except for fringe strategies. Informed Search: Introduced heuristic to estimate of distance to nearest goal for each state and therefore speed up search (Solve performance problem of UCS). Abstraction A state space. A successor function (with actions, costs). A start state and a goal test. Example Traveling in Romania (Map with weighted edge) Pac-man game planning Pancake Problem Properties Complete: Guaranteed to find a solution if one exists? Optimal: Guaranteed to find the least cost path? Time and space complexity. Depth-First Search (DFS) Complete if we prevent cycles Not optimal Time O(b^m) Space O(bm) Breadth-First Search (BFS) Complete Optimal only if costs are all 1 Time O(b^s) Space O(b^s) Uniform Cost Search (UCS) Complete and optimal Time O(b^(C*/E)) Space O(b^(C*/E)) Problem: Explores options in every “direction” because no information about goal location Greedy Search Expand the node that seems closest Can be wrong because of this (not optimal) A*: Combining UCS and GreedyUniform-cost orders by path cost, or backward cost g(n). Greedy orders by goal proximity, or forward cost h(n). A* Search orders by the sum: f(n) = g(n) + h(n). Admissibility Inadmissible (pessimistic) heuristics break optimality by trapping good plans on the fringe Admissible (optimistic) heuristics slow down bad plans but never outweigh true costs A heuristic h is admissible (optimistic) if: 0 &lt;= h(n) &lt;= h(n) where h(n) is the true cost to a nearest goal. ConsistencyHeuristic “arc” cost ≤ actual cost for each arc: h(A) – h(C) ≤ cost(A to C) Heuristic designA* is optimal with admissible/consistent heuristics. In general, most natural admissible heuristics tend to be consistent, especially if from relaxed problems so often use relaxed problems. Comparison Constraint Satisfaction Problems (Identification)Abstraction A special subset of search problems State is defined by variables Xi with values from a domain D (sometimes D depends on i) Goal test is a set of constraints specifying allowable combinations of values for subsets of variables Example Map Coloring N-Queens Cryptarithmetic Sudoku Backtracking SearchBacktracking search is the basic uninformed algorithm for solving CSPs idea 1: One variable at a time: Variable assignments are commutative, so only need to consider assignments to a single variable at each step Idea 2: Check constraints as you go: Consider only values which do not conflict previous assignments. (Might have to do some computation to check the constraints “Incremental goal test”) Improving BacktrackingFilteringDetect inevitable failure early: Keep track of domains for unassigned variables and cross off bad options. Forward CheckingCross off values that violate a constraint when added to the existing assignment. Disadvantage: doesn’t provide early detection for all failures. Constraint PropagationForward checking propagates information from assigned to unassigned variables, but doesn’t provide early detection for all failures. So introduce Arc consistent: An arc X-&gt;Y is consistent iff for every x in the tail there is some y in the head which could be assigned without violating a constraint. A simple form of propagation makes sure all arcs are consistent. Disadvantage: Running slow and can not detect all failures. K-Consistency 1-Consistency (Node Consistency): Each single node’s domain has a value which meets that node’s unary constraints 2-Consistency (Arc Consistency): For each pair of nodes, any consistent assignment to one can be extended to the other K-Consistency: For each k nodes, any consistent assignment to k-1 can be extended to the kth node. Strong K-Consistencyk, k-1…1 Consistency which means we can solve without backtracking. OrderingMinimum remaining values (MRV)Choose the variable with the fewest legal left values in its domain (“Fail-fast” ordering) Least Constraining Value (LCV)Given a choice of variable, choose the least constraining value which leave more choices for future. I.e., the one that rules out the fewest values in the remaining variables. Note that it may take some computation to determine this! (E.g., rerunning filtering) StructureSubproblemsIndependent subproblems are identifiable as connected components of constraint graph. Solve subproblems Independently will speed things up based on # of variables of subproblems. Tree-Structured CSPsIf the constraint graph has no loops, the CSP can be solved in O(n d^2) time compared to general CSPs, where worst-case time is O(d^n) CutsetInstantiate (in all ways) a set of variables such that the remaining constraint graph is a tree structure CSP, which reduce runtime to O((d^c)(n-c)d^2). Local search: Iterative Algorithms for CSPsTake an assignment with unsatisfied constraints, operators reassign variable values until problem solved. PerformanceThe same appears to be true for any randomly-generated CSP except in a narrow range of the ratio Adversarial Search (Minimax Tree) Just like DFS: Time: O(b^m), Space: O(bm). Optimal against a perfect player but do not reasoning against possible outcome. Resource LimitsHard to search all leaves of tree (bound by time). Solution1: Depth limited searchSearch only to a limited depth in the tree, replace terminal utilities with an evaluation function for non-terminal positions. Properties Guarantee of optimal play is gone The deeper in the tree the evaluation function is buried, the less the quality of the evaluation function matters Tradeoff between complexity of features and complexity of computation Evaluation Functions Typically weighted linear sum of features Have danger of replanning Solution2: Alpha-Beta PruningReduce unnecessary compute if possible. General steps(MIN version, MAX version is symmetric) We’re computing the MIN-VALUE at some node n We’re looping over n’s children n’s estimate of the children’s min is dropping Who cares about n’s value? MAX Let a be the best value that MAX can get at any choice point along the current path from the root If n becomes worse than a, MAX will avoid it, so we can stop considering n’s other children (it’s already bad enough that it won’t be played) Properties No effect on minimax value computed for the root! Values of intermediate nodes might be wrong Good child ordering improves effectiveness of pruning A simple example of meta-reasoning: computing about what to compute Uncertainty and UtilitiesUncertain outcomes controlled by chance (EXP node), not an adversary (MIN node). Expectimax SearchReplace MIN node with EXP node MAX nodes as in minimax search. Chance nodes are like MIN nodes but the outcome is uncertain. Calculate their expected utilities. I.e. take weighted average (expectation) of children. Hard to prune if using average. For evaluation, magnitude become important other than ordering. Expectiminmax Search (Mixed Layer)Ad EXP node between MAX and MIN node. Environment is an extra “random agent” player that moves after each min/max agent Each node computes the appropriate combination of its children PropertiesAs depth increases, probability of reaching a given search node shrinks So usefulness of search is diminished So limiting depth is less damaging But pruning is trickier ExampleHistoric AI: TDGammon uses depth-2 search + very good evaluation function + reinforcement learning: world-champion level play. Generalization of minimax Terminals have utility tuples. Node values are also utility tuples. Each player maximizes its own component. Can give rise to cooperation and competition dynamically and naturally. Maximum Expected Utility Principle A rational agent should chose the action that maximizes its expected utility, given its knowledge. Note: an agent can be entirely rational (consistent with MEU) without ever representing or manipulating utilities and probabilities. E.g., a lookup table for perfect tic-tac-toe, a reflex vacuum cleaner Markov Decision Processes (MDP)Agent do not have fully control over future (action). “Markov” generally means that given the present state, the future and the past are independent. Abstraction A set of states s A set of actions a A transition function T(s, a, s’) Probability that a from s leads to s’, i.e., P(s’| s, a) Also called the model or the dynamics A reward function R(s, a, s’) Sometimes just R(s) or R(s’) A start state Maybe a terminal state Quantities Policy = map of states to actions Utility = sum of discounted rewards Values = expected future utility from a state (max node) Q-Values = expected future utility from a q-state (chance node) The value (utility) of a state s: V*(s) = expected utility starting in s and acting optimally The value (utility) of a q-state (s,a): Q*(s,a) = expected utility starting out having taken action a from state s and (thereafter) acting optimally The optimal policy: t*(s) = optimal action from state s Examples Noisy movement Racing car states Utilities of SequencesDiscounting It’s reasonable to maximize the sum of rewards It’s also reasonable to prefer rewards now to rewards later So use discounting: values of rewards decay exponentially Example: discount of 0.5 U([1,2,3]) = 11 + 0.52 + 0.25*3 U([1,2,3]) &lt; U([3,2,1]) Infinite UtilitiesWhat if the game lasts forever? Do we get infinite rewards? Solution Finite horizon: (similar to depth-limited search) Terminate episodes after a fixed T steps (e.g. life) Gives nonstationary policies ( depends on time left) Discounting: use 0 &lt; y &lt; 1 Smaller means smaller “horizon” – shorter term focus Absorbing state: guarantee that for every policy, a terminal state will eventually be reached (like “overheated” for racing) The Bellman Equations Solution1: Value IterationBased on Bellman Equations, fixed depth k and compute the actual value by max over all actions to compute the optimal values: Problems It’s slow – O(A*S^2) per iteration The “max” at each state rarely changes The policy often converges long before the values Example Solution2: Policy IterationBecause of the problem if value iteration, think of using another way to solve MDP. Policy EvaluationFind a way calculate the V’s for a fixed policy. Fixed PoliciesIf we fixed some policy (s), then the tree would be simpler: only one action per state instead of all actions. CalculationIdea 1: Turn recursive Bellman equations into updates (like value iteration) Efficiency: O(S2) per iteration Idea 2: Without the maxes, the Bellman equations are just a linear system Good for parallel Policy Extraction“Reversed” Value Iteration: Find a way to calculate the policy given values. Computing Actions from ValuesDo a mini-expectimax (one step) Computing Actions from Q-ValuesCompletely trivial: just choose the max Q-Values action! ConclusionActions are easier to select from Q-Values than values Steps Policy evaluation: calculate utilities for some fixed policy (not optimal utilities!) until convergence Policy improvement: update policy using one-step look-ahead with resulting converged (but not optimal!) utilities as future values Repeat steps until policy converges Properties It’s still optimal! Can converge (much) faster under some conditions Look in depth: These all look the same! They basically are – they are all variations of Bellman updates They all use one-step lookahead expectimax fragments They differ only in whether we plug in a fixed policy or max over actions From MDP to learningSpecifically, reinforcement learning is an MDP, but you couldn’t solve it with just computation (probability unknown). So you needed to actually act to figure it out. Exploration: you have to try unknown actions to get information Exploitation: eventually, you have to use what you know Regret: even if you learn intelligently, you make mistakes Sampling: because of chance, you have to try things repeatedly Difficulty: learning can be much harder than solving a known MDP Reinforcement LearningAbstraction Still assume a Markov decision process (MDP): A set of states s S A set of actions (per state) A A model T(s,a,s’) A reward function R(s,a,s’) Still looking for a policy (s) New twist: don’t know T or R, must actually try actions and states out to learn Model-Based LearningModel-Based IdeaLearn an approximate model based on experiences。 Solve for values as if the learned model were correct Steps Learn empirical MDP model Count outcomes s’ for each s, a Normalize to give an estimate of T Discover each R when we experience (s, a, s’) Solve the learned MDP For example, use value iteration, as before Example Model-Free LearningPassive Reinforcement Learning Simplified task: policy evaluation Input: a fixed policy (s) You don’t know the transitions T(s,a,s’) You don’t know the rewards R(s,a,s’) Goal: learn the state values Active Reinforcement Learning Full reinforcement learning: optimal policies (like value iteration) You don’t know the transitions T(s,a,s’) You don’t know the rewards R(s,a,s’) You choose the actions now Goal: learn the optimal policy / values In this case: Learner makes choices! Fundamental tradeoff: exploration vs. exploitation This is NOT offline planning! You actually take actions in the world and find out what happens… Direct EvaluationGoalCompute values for each state under IdeaAverage together observed sample values Act according to Every time you visit a state, write down what the sum of discounted rewards turned out to be Average those samples Example ProblemsAdvantage It’s easy to understand It doesn’t require any knowledge of T, R It eventually computes the correct average values, using just sample transitions Disadvantage It wastes information about state connections Each state must be learned separately So, it takes a long time to learn Temporal Difference LearningIdeaSample-Based Policy Evaluation. Learn from every experience. Update V(s) each time we experience a transition (s, a, s’, r) Likely outcomes s’ will contribute updates more often Example ProblemsTD value leaning is a model-free way to do policy evaluation, mimicking Bellman updates with running sample averages. However, if we want to turn values into a (new) policy, we’re sunk. Solution: learn Q-values, not values. Q-LearningIdea &amp; Steps Properties Converges to optimal policy – even if you’re acting suboptimally Caveats Have to explore enough. Have to eventually make the learning rate small enough but not decrease it too quickly. Basically, in the limit, it doesn’t matter how you select actions.]]></content>
      <categories>
        <category>Notes</category>
        <category>Computer Science</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes of Predictably Irrational]]></title>
    <url>%2FIrrational%2F</url>
    <content type="text"><![CDATA[My notes of duke online course dan ariely - predictably irrational Irrationality Our decisions are influenced by environment, defaults and complexity. Defaults are neither good nor bad, and it’s everywhere, we should think on when they work for us and when they not. The path of least resistance is likely when deviating from the default is complex. It’s hard for us to make evaluation Decisions are influenced by choice set Initial decision will influenced future decisions, self herding, so doubt your intuitions The psychology of moneyOpportunity costWe place higher value on specific items than the money value of those items. This is because people tend to take shortcuts when problems are complex. (Do not consider real opportunity cost) Example:$1000 Pioneer stereo VS $700 Sony stereo VS $1000 Sony stereo with $300 CD coupon Solution:We should think money in terms of opportunity cost: what you give up for choosing one thing over another RelativityWe think money in relative other than absolute. Example: Save $8 on a $16 pen VS save $8 on a $1008 projector Add $2000 for a leather seat for $39,000 car VS Add $2000 for a leather seat for $500 office seat Add feature when renovate house VS select tomato in market Earn 90K as lowest VS earn 80k as highest in different company Solution:Control range of comparison, think of absolute value. Pain of payingThere is a moral tax to consumption. Timing and method of payment affect enjoyment. Example: Pay with credit card over cash Prepaid vacation Gift remove the pain of payment Chips in casino AOL’s switch to unlimited plan quadrupled user’s usage. Application:Mortgage, User prediction, gift Mental accountingWhen money is for a specific category, we value it differently: more easily be spent. Example: Lost a ticket over same amount of money Saving interest rate and loan interest rate Solution:Partition our spending in advance and keep optimize it. Fairness and ReciprocityJudgement of fair price depends on perceived cost. We use this measurement because determine the real value is hard for us and we use shortcuts (based on effort). Example: Giving a tip to lock locksmith who broke your lock after 2 hours over the one unlock it within 2 minutes without broking it. Pay for ATM over a banker. A beer from hotel or grocery store. Solution:Express your effort. Loss aversion and endowment effectGains make us happy but losses make us really miserable. And once you own something, you start to value more on what you will give up and less on what you will gain by exchanging it. Example: Prepaid commission for sales students who have a mug not want to exchange it for a candy from students who have candy, same for the opposite Application: 401k matching from company 30 days money back guarantee Market and and social normsGifts build up social capital, while money reduce it Example: Volunteer job vs paid job Benefits of company Late fine for parents if the pick up their kid too late. Small fine actually make parents more likely to be late. Complete contract reduce social norm, and make people hard to agree on the sprit of contact. Solution:Think of where you are at the range of social norm to market norm and if it is suitable. Micro payments Keep price same make us easier to buy Consolidate multiple purchases reduce pain of paying Pre-paid option reduce micro-payment and reduce pain of paying Free version increate the barrier of purchase, largely Example: iTunes price are all $0.99 the mircopricing experiment DishonestyThe model of rational crime Lots of people cheat a little bit. People will weight rewards of cheating, possibility of being caught and punishment (Becker model: cost benefits analyses). However, dishonesty does not solely rely on cost benefits analyses. So we should consider fudge factor: People will consider themselves moral if they cheat only a little bit. Shrinking and expanding the fudge factorCheating is all about how we rationalize it. Some example of manipulate it:Shrink Being reminded of morality (however, remind afterward will not work). Expand Increase distant from money. Social acceptance (seeing others from same social group cheat). Creativity (Create story to defend yourself) Solution:Control small crimes, lots of small crimes may make a big difference. Conflicts of interestConflicts of interest operate within the fudge factor: People tend to rationalize towards benefits/rewards. Example: Data manipulate in science research. People like the art from gallery who give cash reward. Solution:Try to eliminate the result of interest conflict Cheating Over time and across culture People start to cheat a little and at some point, they start to cheat a lot. Confession provide a way to reset, which cut down the way to cheat more. People shared the same basic fudge factor across cultures. However, culture can shrink/expand fudge factor on a specific domain. Application:Reconciliation act for South Africa Guest speak Medical decisions: People tend to make different decision for their own than for others. (Peter Ubel) Moral pendulum: People tend to make up after doing bad / indulge after doing good. (Nina Mazar) Labor and MotivationWe are motivated to do things that we find meaningful. Sisyphus condition (Meaning)Doing same job repeatedly without meaning will be demotivate. And in this case, their internal love for this procedure will not reduce their demotivation. Acknowledgement It is relatively easy to make people feel good about their work. (Just a simple acknowledgement) In contrast, simply ignoring people can be as demotivating as destroying their work. SolutionInject more meaning to various situation. IKEA effect (Labor) Labor lead to love. Builder mistakenly think everyone will love it. Labor lead to love only when builder finish the task and create something. (Too much effort will have negative effect) Example Your own child. Self-designed T-shirt/shoes. Girls chased by boys ApplicationCustomization is more about preference, but the effort people put in. Not-invented-here biasIKEA effect on idea: people love their solution than others. SolutionWe should love what we do but now too much. Cognitive DissonanceWe behave one way but don’t believe in the same way. So we shift our belief afterwards. Example Boring task with high pay is boring while low pay is more interesting (people shift because they don’t want to admit they are doing boring job for low payment) Zappos hiring process (People convince they love the company because they give up $2000 by deciding join the company) Monetary Stress and Performance Bonus’s effect follows Yerkes-Dodson Curve except it is only for pure mechanical task. Keep getting bonus will lead to future loss aversion and continue to decrease performance. SolutionA state of “flow” drive the best performance (immersed in task). InspirationIs current high-tech company’s bonus system working? Social Stress and PerformanceHigher motivation does not necessarily result in better performance: Anxiety caused by public pressure impedes performance. ExampleNBA player have lower free throw rate in the last 5 minutes. Bonus, Labor and Motivation (Summary)Bonus:Small amount of money can move relationship from social to financial domain. Large amount can increase motivation but decrease performance. And we do not operate by simple rules of rewards but multiple combination. Motivation:Current world have more potential to find the meaning of our work. So A production line environment may not be as efficient as before. And people are less easy to be supervised. Solution Think of motivation not only by money: A thoughtful gift can mean much more than money bonus. Make work has meaning and trust employee to do the right thing. So that people will connect to the output. Guest speak (Lailn Anik) At different stage of life, motivation change. When facing different type task, bonus and pressure work differently. Self-controlDifficulty with self-controlPresent focus bias: Give more weight to the current state and environment. Example: Global warming and how to incentive people to help environment. Toyota Prius: Social rewards of hybrid driving. Solution Reward Substitution: Using an alternate reward that is intermediate and more motivating. Ulysses (Self-control) Contracts: Enforce self-control by removing the tempting. Distracted yourself from temptations. Eliminate decision point. (Religious people do not think of smoking on Sabbath) Maximize Reward Lotteries Randomization Counterfactuals Regret Reward Substitution FreedomFreedom reduce power of Ulysses Contracts. We must find a balance between the amount of freedom we crave and the control we need to shield us from tempting. Emotion Basic part of human. Can overcome cognition. Can work both for or against us. Two systemsThe limbic systems: Similar across all animals.The cognitive system: Mark us as human. Example Emotion are transient and more short-live than we expect. Emotion can over take cognition. Example: People being sexual aroused solutionUnderstand how emotion can change us. Intro-empathy problemWe making decision under a specific emotional state, we have Intro-empathy gap: Thinking we will have the same emotional state in the future Example Movie choice diffs at different emotion. The thirsty you are, the more likely you will donate to charity. The Identifiable Victim EffectA single victim inspire action while generation information about massive victims does not (statistic information inspire cognitive thinking) CauseThe discrepancy between cognitive and emotional thinking. Example The mismatch between actual need and money donated to each event/disease. The trolley problem. Donation for people in another side of the world. SolutionPortray right information (Break big problem into small pieces) to people if you want to convey them. Emotional Decision MakingFor products that are consumed emotionally, lack of cognitive input can lead to increased enjoyment. ExampleBuying jams without knowing any cognitive information (price, brand etc). SolutionThe decision environment should match up with the consumption environment. ExampleBuying ugly speaker with better sound VS good-looking speaker with worse sound. Risk AssessmentWe value risk higher when: Recently being reminded (salient in memory). We have no control. We have emotional response. Example Kill by shark (reminded by lots of movies) VS by airplane wreckage drop from sky Terrorist (lack of control) VS Car accident MindnodeConverting Markdown to a mind map LinksBook Notes of Predictably Irrational]]></content>
      <categories>
        <category>Notes</category>
        <category>Psychology</category>
      </categories>
      <tags>
        <tag>Predictably Irrational</tag>
        <tag>Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CO-OP at insightFinder]]></title>
    <url>%2FWork-as-entry-level-engineer-at-start-up%2F</url>
    <content type="text"><![CDATA[After my summer internship at Facebook, I started to work at insightFinder during my second master’s year. As a star-up company, insightFinder gave me a lot of different experience than working at a large tech company such as Facebook, and taught different things. The StoryOnboardingStarting to work is quite simple here. One of my friend as well as classmate took me to our office on my first date to work. He worked here during summer and will continue for the following semester, and he will be the one to guide me. We had lunch together with two other co-workers and they introduced our overall infrastructure on the first day. Then I spent most of the day setting up my environment and making things running. Our stack includes Java backend, javascript UI, RabbitMQ, in-house C++ machine learning library and several agents on Splunk, AWS and Logstash. I was mainly focus on the backend development at that time and got a easy-fix for warm up. Struggle as a noobAt the beginning, I spent most of time learning and discussing. As I got familiar with the development cycle, I was soon assigned to improve an existing feature. This is the time I really struggle with. I know the overall structure, but I hardly understand the code base, which is to me poorly maintained and documented. I have to ask people around about how things work and where should I start with, and no one knows everything! Sometimes I have to call previous employee to get the answer and even our CEO had taught me for several times. After struggling for some days I got to know how backend is working here, and began to make progress on each tasks. Well, months later I realized the quality of work at this time is not good, code are buggy and not well formatted, and I have to rewrite some of them. But still, it feels good when making progress. Make progress and celebrateThough struggled, I was learning and finishing projects one after another. Gradually I realized there are parts of the system I was responsible for, and as a member in a small but busy team, that means you may working on things beyond coding. I started joining meetings with customers, writing user guides and things like that. It is interesting to know things like these and I feel close to the company. In the mean time the team is also making progress. We got new customers and demos came one after another. And extra hours always needed before a demo day. Life is stressful but rewarding. Good news is a new investment allows us to expand the team. We some both new-grad and senior engineers coming to reduce the burden. And a bigger office is awaiting. Dive deep with experienceAfter the first semester, I kept working on winter break. Since then I worked not only on individual feature projects but also on infrastructure design, performance improvement and reliability improve. I got the chance to resolve several data processing bottleneck which is much more challenging and interesting. Also, there are new coming engineers. And we started to enhance our code review as well as CI flow. so I started to answer questions instead of asking. I need to explain some of my work as well and assign some tasks to interns. Trying to explain and help drove me to re-think and I got deeper understanding, and make me feel I am not a noob anymore. LeavingGraduation means farewell. Last few weeks is still busy but I put some time to turn some work into a framework and hand over my things to another full time. And kept answering questions and saying goodbye by phone after leaving office. It’s my fortune to be part on this company and really hope these guys have a bright future. Some ThoughtsWhy an existing system should sucksEvery company have old, existing system and most of time we will work based on it. I was like to complain of it, as it looks big, merely maintainable, and hard to use. However, I gradually realized maybe it was good, and expandable enough when it was first developed. It only became a monster when our needs changed, or people left or simply forgot. And when we trying to replace it with a new system, the same complains started after some times because of the same reason. How to resolve this? I am not sure, maybe just kept a good development practice and a peaceful attitude when you encounter a problem. Before answer a tech questionI got a lot of questions to answer when new engineer came into team. Some days I feel like my main job is to talk and answer various questions, from infra design, code base to why a little bug happened, how to use a tool. Then I found there is something wrong with it, some questions are too simple that I don’t even need to think to give the answer, some however, I have no clue to know the answer. And sometimes, even if I have a answer, the questioner came back with another question completely different to the first one. So I start to think of how to make this communication more efficient so that I can still have time on my own work. I began to ask questioner some questions before they start their questioning: What is the task they asked you to solve? What are you working on to solve it? Do you have better plan or another idea? It is surprising efficient, most of the time they found the solution themselves, or they realized they are heading for a wrong direction. This save me the energy of doing context switch for answer a specific questions and can get back to my work easier. Prepare for meetings and reportsMeetings here are more frequent, and more roles are attending. CEO, tech lead, designer, sales, and even customer. I am not a good talker and I feel sad when my explanation is misunderstood or ignored. So make sure I kept these in mind when reporting: Make sure everyone have the same context, introduce previous context if someone is missing last time, update terms and make sure everyone is using the same. Stop taking about things others can’t understand. Not everyone knows “CPU bottleneck”, “network throughput”. Use “the user need to wait for around 10 minutes because our sever have a performance problem” instead of “our message queue is blocked because of CPU bottleneck” Kept the key point first. What problem you found, and what solution you are trying. You will lose audience when you start with introducing the root cause first. Stop meaningless talk. “It can not be done”, “It is wrong”, “It can not happened”. No one wants to hear them, talk about solution, reason instead. Start-up vs Big companyThere is much more freedom in a start-up company like this. We move faster than Facebook and it’s very exciting to see my own work delivered for a big impact. However, sometimes it is too busy and I feel a little tried. And sometimes I have work on several tasks in parallel and context switch is causing me pain and make me stressful. And there is less infrastructure to help us with the quality of work. The PicsComing soon]]></content>
      <categories>
        <category>Work experience</category>
      </categories>
      <tags>
        <tag>insightFinder</tag>
        <tag>internship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Personal Blog based on Hexo + NexT]]></title>
    <url>%2Fmy-site-development-log-hexo-nexT-GithubPage%2F</url>
    <content type="text"><![CDATA[I played with hexo to build my first personal website on 2016. Then it was untouched for a long time until today, I decided to pick things up and try to learn some UI staff with it. This page is mainly for recording what I learnt and and how I used them on this site including 3rd party plugins, theme modification and SEO. Add pluginsGitment commentsGitment is a comment system based on GitHub Issues, which can be used in the frontend without any server-side implementation. Gitment plugin is supported in current NexT(v5.1.4), just need a few steps to set up: Create OAuth application Go to settings of your github account Go to Developer settings Click Register a new application Fill in: 1234Application name：GitmentHomepage URL：&lt;your website link&gt;Application description：&lt;Anything&gt;Authorization callback URL：&lt;your website link&gt; Copy Client ID and Client Secret for future use Create repository for gitment Create a new repository named gitment-comments Change NexT config Search gitment in config.yml12345678910111213gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide &apos;Powered by ...&apos; on footer, and more language: # Force language, or auto switch by theme github_user: # MUST HAVE, Your Github ID github_repo: `gitment-comments` # MUST HAVE, The repo you use to store Gitment comments client_id: `Client ID` # MUST HAVE, Github client id for the Gitment client_secret: `Client Secret` # EITHER this or proxy_gateway, Github access secret token for the Gitment proxy_gateway: # Address of api proxy, See: https://github.com/aimingoo/intersect redirect_protocol: # Protocol of redirect_uri with force_redirect_protocol when mint enabled Add various plugins based on hexo document baidu analytics addThis share leancloud_visitors Local Search Customize pageTopX: Hottest post page (Need Leancloud support)Reference：5.8 添加 TopX 页面Basically it create a new page and rearranged post based on views from Leancloud’s database we created when we add leancloud_visitors plugin. Create new page 1hexo new page &quot;top&quot; Add menu in theme configuration 12menu: top: /top/ || signal Modify the new index.md for “top” page: 1234567891011121314151617181920212223242526272829---title: TopXcomments: falsekeywords: top,文章阅读量排行榜---&lt;div id=&quot;top&quot;&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js&quot;&gt;&lt;/script&gt;&lt;script&gt;AV.initialize(&quot;app_id&quot;, &quot;app_key&quot;);&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; var time=0 var title=&quot;&quot; var url=&quot;&quot; var query = new AV.Query(&apos;Counter&apos;); query.notEqualTo(&apos;id&apos;,0); query.descending(&apos;time&apos;); query.limit(1000); query.find().then(function (todo) &#123; for (var i=0;i&lt;1000;i++)&#123; var result=todo[i].attributes; time=result.time; title=result.title; url=result.url; var content=&quot;&lt;a href=&apos;&quot;+&quot;https://reuixiy.github.io&quot;+url+&quot;&apos;&gt;&quot;+title+&quot;&lt;/a&gt;&quot;+&quot;&lt;br /&gt;&quot;+&quot;&lt;font color=&apos;#555&apos;&gt;&quot;+&quot;阅读次数：&quot;+time+&quot;&lt;/font&gt;&quot;+&quot;&lt;br /&gt;&lt;br /&gt;&quot;; document.getElementById(&quot;top&quot;).innerHTML+=content &#125; &#125;, function (error) &#123; console.log(&quot;error&quot;); &#125;);&lt;/script&gt; Replace variables: Change app_id and app_key in index.md for your own leancloud id. Also you can change TopX’s X by changing this line: query.limit(1000); Learning sourceHexoHexo搭建博客教程Hexo+NexT 博客搭建相册打造个性超赞博客Hexo+NexT+GithubPages的超深度优化hexo的next主题个性化配置教程hexo教程 Version controlUsing Git Submodules to Manage Your Custom Hexo Theme SEOhexo高阶教程：想让你的博客被更多的人在搜索引擎中搜到吗动动手指，不限于NexT主题的Hexo优化（SEO篇Hexo Seo优化让你的博客在google搜索排名第一 CautionsConfig overrideWe should be careful with config override. If you want to override one item from a section, you should include other items of that section. For example, once I forget to declare other sections except “display” for sidebar config:12sidebar: display: always This cause the theme config is missing other sections including “offset” which is critical to sidebar affix. So I have a wrongly placed sidebar. I should change to following config if I want to use sidebar:1234567sidebar: position: left display: always offset: 12 b2t: false scrollpercent: false onmobile: false To do add timeline on about page Add Chinese post page Improve config override, it cause pain in the ass! Add SEO (Baidu + Google) Add TopX page Add photo of post and correct photo links]]></content>
      <categories>
        <category>UI</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>nextT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to do dynamic programming]]></title>
    <url>%2Fdp%2F</url>
    <content type="text"><![CDATA[The key for dp is to find the variables to represent the states and deduce the transition function. (aka find state -&gt; find relation -&gt; find constraints) Take Best Time to Buy and Sell Stock with Cooldown as an example: Find stateThe natural states for this problem is the 3 possible transactions : buy, sell, rest. Because the transaction sequences can end with any of these three states. For each of them we make an array, buy[n], sell[n] and rest[n]., which represents the maxProfit for any sequence end with buy/sell/rest. Find relationThen we want to deduce the transition functions for buy sell and rest. By definition we have: buy[i] = max(rest[i-1]-price, buy[i-1]) sell[i] = max(buy[i-1]+price, sell[i-1]) rest[i] = max(sell[i-1], buy[i-1], rest[i-1])Where price is the price of day i. All of these are very straightforward. They simply represents : We have to rest before we buy and we have to buy before we sell Find constraintsOne tricky point is how do you make sure you sell before you buy, since from the equations it seems that [buy, rest, buy] is entirely possible. Well, the answer lies within the fact that buy[i] &lt;= rest[i] which means rest[i] = max(sell[i-1], rest[i-1]). That made sure [buy, rest, buy] is never occurred. A further observation is that and rest[i] &lt;= sell[i] is also true therefore rest[i] = sell[i-1]Substitute this in to buy[i] we now have 2 functions instead of 3: buy[i] = max(sell[i-2]-price, buy[i-1]) sell[i] = max(buy[i-1]+price, sell[i-1]) This is better than 3, but, we can do even better Since states of day i relies only on i-1 and i-2 we can reduce the O(n) space to O(1). And here we are at our final solution: 1234567891011public int maxProfit(int[] prices) &#123; int sell = 0, prev_sell = 0, buy = Integer.MIN_VALUE, prev_buy; for (int price : prices) &#123; prev_buy = buy; buy = Math.max(prev_sell - price, prev_buy); prev_sell = sell; sell = Math.max(prev_buy + price, prev_sell); &#125; return sell;&#125;]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Algorithms</tag>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
</search>
