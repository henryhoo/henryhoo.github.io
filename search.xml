<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Notes of cs188 Artificial Intelligence]]></title>
    <url>%2Fcs188%2F</url>
    <content type="text"><![CDATA[My notes of UC Berkeley online course Intro to Artificial Intelligence SearchTwo type of searchPlanning: sequences of actions The path to the goal is the important thing Paths have various costs, depths Heuristics give problem-specific guidance Identification: assignments to variables The goal itself is important, not the path All paths at the same depth (for some formulations) CSPs are specialized for identification problems Uninformed and Informed Search (Planning) All search algorithms are the same except for fringe strategies. Informed Search: Introduced heuristic to estimate of distance to nearest goal for each state and therefore speed up search (Solve performance problem of UCS). Abstraction A state space. A successor function (with actions, costs). A start state and a goal test. Example Traveling in Romania (Map with weighted edge) Pac-man game planning Pancake Problem Properties Complete: Guaranteed to find a solution if one exists? Optimal: Guaranteed to find the least cost path? Time and space complexity. Depth-First Search (DFS) Complete if we prevent cycles Not optimal Time O(b^m) Space O(bm) Breadth-First Search (BFS) Complete Optimal only if costs are all 1 Time O(b^s) Space O(b^s) Uniform Cost Search (UCS) Complete and optimal Time O(b^(C*/E)) Space O(b^(C*/E)) Problem: Explores options in every “direction” because no information about goal location Greedy Search Expand the node that seems closest Can be wrong because of this (not optimal) A*: Combining UCS and GreedyUniform-cost orders by path cost, or backward cost g(n). Greedy orders by goal proximity, or forward cost h(n). A* Search orders by the sum: f(n) = g(n) + h(n). Admissibility Inadmissible (pessimistic) heuristics break optimality by trapping good plans on the fringe Admissible (optimistic) heuristics slow down bad plans but never outweigh true costs A heuristic h is admissible (optimistic) if: 0 &lt;= h(n) &lt;= h(n) where h(n) is the true cost to a nearest goal. ConsistencyHeuristic “arc” cost ≤ actual cost for each arc: h(A) – h(C) ≤ cost(A to C) Heuristic designA* is optimal with admissible/consistent heuristics. In general, most natural admissible heuristics tend to be consistent, especially if from relaxed problems so often use relaxed problems. Comparison Constraint Satisfaction Problems (Identification)Abstraction A special subset of search problems State is defined by variables Xi with values from a domain D (sometimes D depends on i) Goal test is a set of constraints specifying allowable combinations of values for subsets of variables Example Map Coloring N-Queens Cryptarithmetic Sudoku Backtracking SearchBacktracking search is the basic uninformed algorithm for solving CSPs idea 1: One variable at a time: Variable assignments are commutative, so only need to consider assignments to a single variable at each step Idea 2: Check constraints as you go: Consider only values which do not conflict previous assignments. (Might have to do some computation to check the constraints “Incremental goal test”) Improving BacktrackingFilteringDetect inevitable failure early: Keep track of domains for unassigned variables and cross off bad options. Forward CheckingCross off values that violate a constraint when added to the existing assignment. Disadvantage: doesn’t provide early detection for all failures. Constraint PropagationForward checking propagates information from assigned to unassigned variables, but doesn’t provide early detection for all failures. So introduce Arc consistent: An arc X-&gt;Y is consistent iff for every x in the tail there is some y in the head which could be assigned without violating a constraint. A simple form of propagation makes sure all arcs are consistent. Disadvantage: Running slow and can not detect all failures. K-Consistency 1-Consistency (Node Consistency): Each single node’s domain has a value which meets that node’s unary constraints 2-Consistency (Arc Consistency): For each pair of nodes, any consistent assignment to one can be extended to the other K-Consistency: For each k nodes, any consistent assignment to k-1 can be extended to the kth node. Strong K-Consistencyk, k-1…1 Consistency which means we can solve without backtracking. OrderingMinimum remaining values (MRV)Choose the variable with the fewest legal left values in its domain (“Fail-fast” ordering) Least Constraining Value (LCV)Given a choice of variable, choose the least constraining value which leave more choices for future. I.e., the one that rules out the fewest values in the remaining variables. Note that it may take some computation to determine this! (E.g., rerunning filtering) StructureSubproblemsIndependent subproblems are identifiable as connected components of constraint graph. Solve subproblems Independently will speed things up based on # of variables of subproblems. Tree-Structured CSPsIf the constraint graph has no loops, the CSP can be solved in O(n d^2) time compared to general CSPs, where worst-case time is O(d^n) CutsetInstantiate (in all ways) a set of variables such that the remaining constraint graph is a tree structure CSP, which reduce runtime to O((d^c)(n-c)d^2). Local search: Iterative Algorithms for CSPsTake an assignment with unsatisfied constraints, operators reassign variable values until problem solved. PerformanceThe same appears to be true for any randomly-generated CSP except in a narrow range of the ratio Adversarial Search (Minimax Tree) Just like DFS: Time: O(b^m), Space: O(bm). Optimal against a perfect player but do not reasoning against possible outcome. Resource LimitsHard to search all leaves of tree (bound by time). Solution1: Depth limited searchSearch only to a limited depth in the tree, replace terminal utilities with an evaluation function for non-terminal positions. Properties Guarantee of optimal play is gone The deeper in the tree the evaluation function is buried, the less the quality of the evaluation function matters Tradeoff between complexity of features and complexity of computation Evaluation Functions Typically weighted linear sum of features Have danger of replanning Solution2: Alpha-Beta PruningReduce unnecessary compute if possible. General steps(MIN version, MAX version is symmetric) We’re computing the MIN-VALUE at some node n We’re looping over n’s children n’s estimate of the children’s min is dropping Who cares about n’s value? MAX Let a be the best value that MAX can get at any choice point along the current path from the root If n becomes worse than a, MAX will avoid it, so we can stop considering n’s other children (it’s already bad enough that it won’t be played) Properties No effect on minimax value computed for the root! Values of intermediate nodes might be wrong Good child ordering improves effectiveness of pruning A simple example of meta-reasoning: computing about what to compute Uncertainty and UtilitiesUncertain outcomes controlled by chance (EXP node), not an adversary (MIN node). Expectimax SearchReplace MIN node with EXP node MAX nodes as in minimax search. Chance nodes are like MIN nodes but the outcome is uncertain. Calculate their expected utilities. I.e. take weighted average (expectation) of children. Hard to prune if using average. For evaluation, magnitude become important other than ordering. Expectiminmax Search (Mixed Layer)Ad EXP node between MAX and MIN node. Environment is an extra “random agent” player that moves after each min/max agent Each node computes the appropriate combination of its children PropertiesAs depth increases, probability of reaching a given search node shrinks So usefulness of search is diminished So limiting depth is less damaging But pruning is trickier ExampleHistoric AI: TDGammon uses depth-2 search + very good evaluation function + reinforcement learning: world-champion level play. Generalization of minimax Terminals have utility tuples. Node values are also utility tuples. Each player maximizes its own component. Can give rise to cooperation and competition dynamically and naturally. Maximum Expected Utility Principle A rational agent should chose the action that maximizes its expected utility, given its knowledge. Note: an agent can be entirely rational (consistent with MEU) without ever representing or manipulating utilities and probabilities. E.g., a lookup table for perfect tic-tac-toe, a reflex vacuum cleaner Markov Decision Processes (MDP)Agent do not have fully control over future (action). “Markov” generally means that given the present state, the future and the past are independent. Abstraction A set of states s A set of actions a A transition function T(s, a, s’) Probability that a from s leads to s’, i.e., P(s’| s, a) Also called the model or the dynamics A reward function R(s, a, s’) Sometimes just R(s) or R(s’) A start state Maybe a terminal state Quantities Policy = map of states to actions Utility = sum of discounted rewards Values = expected future utility from a state (max node) Q-Values = expected future utility from a q-state (chance node) The value (utility) of a state s: V*(s) = expected utility starting in s and acting optimally The value (utility) of a q-state (s,a): Q*(s,a) = expected utility starting out having taken action a from state s and (thereafter) acting optimally The optimal policy: t*(s) = optimal action from state s Examples Noisy movement Racing car states Utilities of SequencesDiscounting It’s reasonable to maximize the sum of rewards It’s also reasonable to prefer rewards now to rewards later So use discounting: values of rewards decay exponentially Example: discount of 0.5 U([1,2,3]) = 11 + 0.52 + 0.25*3 U([1,2,3]) &lt; U([3,2,1]) Infinite UtilitiesWhat if the game lasts forever? Do we get infinite rewards? Solution Finite horizon: (similar to depth-limited search) Terminate episodes after a fixed T steps (e.g. life) Gives nonstationary policies ( depends on time left) Discounting: use 0 &lt; y &lt; 1 Smaller means smaller “horizon” – shorter term focus Absorbing state: guarantee that for every policy, a terminal state will eventually be reached (like “overheated” for racing) The Bellman Equations Solution1: Value IterationBased on Bellman Equations, fixed depth k and compute the actual value by max over all actions to compute the optimal values: Problems It’s slow – O(A*S^2) per iteration The “max” at each state rarely changes The policy often converges long before the values Example Solution2: Policy IterationBecause of the problem if value iteration, think of using another way to solve MDP. Policy EvaluationFind a way calculate the V’s for a fixed policy. Fixed PoliciesIf we fixed some policy (s), then the tree would be simpler: only one action per state instead of all actions. CalculationIdea 1: Turn recursive Bellman equations into updates (like value iteration) Efficiency: O(S2) per iteration Idea 2: Without the maxes, the Bellman equations are just a linear system Good for parallel Policy Extraction“Reversed” Value Iteration: Find a way to calculate the policy given values. Computing Actions from ValuesDo a mini-expectimax (one step) Computing Actions from Q-ValuesCompletely trivial: just choose the max Q-Values action! ConclusionActions are easier to select from Q-Values than values Steps Policy evaluation: calculate utilities for some fixed policy (not optimal utilities!) until convergence Policy improvement: update policy using one-step look-ahead with resulting converged (but not optimal!) utilities as future values Repeat steps until policy converges Properties It’s still optimal! Can converge (much) faster under some conditions Look in depth: These all look the same! They basically are – they are all variations of Bellman updates They all use one-step lookahead expectimax fragments They differ only in whether we plug in a fixed policy or max over actions From MDP to learningSpecifically, reinforcement learning is an MDP, but you couldn’t solve it with just computation (probability unknown). So you needed to actually act to figure it out. Exploration: you have to try unknown actions to get information Exploitation: eventually, you have to use what you know Regret: even if you learn intelligently, you make mistakes Sampling: because of chance, you have to try things repeatedly Difficulty: learning can be much harder than solving a known MDP Reinforcement LearningAbstraction Still assume a Markov decision process (MDP): A set of states s S A set of actions (per state) A A model T(s,a,s’) A reward function R(s,a,s’) Still looking for a policy (s) New twist: don’t know T or R, must actually try actions and states out to learn Model-Based LearningModel-Based IdeaLearn an approximate model based on experiences。 Solve for values as if the learned model were correct Steps Learn empirical MDP model Count outcomes s’ for each s, a Normalize to give an estimate of T Discover each R when we experience (s, a, s’) Solve the learned MDP For example, use value iteration, as before Example Model-Free LearningPassive Reinforcement Learning Simplified task: policy evaluation Input: a fixed policy (s) You don’t know the transitions T(s,a,s’) You don’t know the rewards R(s,a,s’) Goal: learn the state values Active Reinforcement Learning Full reinforcement learning: optimal policies (like value iteration) You don’t know the transitions T(s,a,s’) You don’t know the rewards R(s,a,s’) You choose the actions now Goal: learn the optimal policy / values In this case: Learner makes choices! Fundamental tradeoff: exploration vs. exploitation This is NOT offline planning! You actually take actions in the world and find out what happens… Direct EvaluationGoalCompute values for each state under IdeaAverage together observed sample values Act according to Every time you visit a state, write down what the sum of discounted rewards turned out to be Average those samples Example ProblemsAdvantage It’s easy to understand It doesn’t require any knowledge of T, R It eventually computes the correct average values, using just sample transitions Disadvantage It wastes information about state connections Each state must be learned separately So, it takes a long time to learn Temporal Difference LearningIdeaSample-Based Policy Evaluation. Learn from every experience. Update V(s) each time we experience a transition (s, a, s’, r) Likely outcomes s’ will contribute updates more often Example ProblemsTD value leaning is a model-free way to do policy evaluation, mimicking Bellman updates with running sample averages. However, if we want to turn values into a (new) policy, we’re sunk. Solution: learn Q-values, not values. Q-LearningIdea &amp; Steps Properties Converges to optimal policy – even if you’re acting suboptimally Caveats Have to explore enough. Have to eventually make the learning rate small enough but not decrease it too quickly. Basically, in the limit, it doesn’t matter how you select actions.]]></content>
      <categories>
        <category>Notes</category>
        <category>Computer Science</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
        <tag>Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes of Predictably Irrational]]></title>
    <url>%2FIrrational%2F</url>
    <content type="text"><![CDATA[My notes of duke online course dan ariely - predictably irrational Irrationality Our decisions are influenced by environment, defaults and complexity. Defaults are neither good nor bad, and it’s everywhere, we should think on when they work for us and when they not. The path of least resistance is likely when deviating from the default is complex. It’s hard for us to make evaluation Decisions are influenced by choice set Initial decision will influenced future decisions, self herding, so doubt your intuitions The psychology of moneyOpportunity costWe place higher value on specific items than the money value of those items. This is because people tend to take shortcuts when problems are complex. (Do not consider real opportunity cost) Example:$1000 Pioneer stereo VS $700 Sony stereo VS $1000 Sony stereo with $300 CD coupon Solution:We should think money in terms of opportunity cost: what you give up for choosing one thing over another RelativityWe think money in relative other than absolute. Example: Save $8 on a $16 pen VS save $8 on a $1008 projector Add $2000 for a leather seat for $39,000 car VS Add $2000 for a leather seat for $500 office seat Add feature when renovate house VS select tomato in market Earn 90K as lowest VS earn 80k as highest in different company Solution:Control range of comparison, think of absolute value. Pain of payingThere is a moral tax to consumption. Timing and method of payment affect enjoyment. Example: Pay with credit card over cash Prepaid vacation Gift remove the pain of payment Chips in casino AOL’s switch to unlimited plan quadrupled user’s usage. Application:Mortgage, User prediction, gift Mental accountingWhen money is for a specific category, we value it differently: more easily be spent. Example: Lost a ticket over same amount of money Saving interest rate and loan interest rate Solution:Partition our spending in advance and keep optimize it. Fairness and ReciprocityJudgement of fair price depends on perceived cost. We use this measurement because determine the real value is hard for us and we use shortcuts (based on effort). Example: Giving a tip to lock locksmith who broke your lock after 2 hours over the one unlock it within 2 minutes without broking it. Pay for ATM over a banker. A beer from hotel or grocery store. Solution:Express your effort. Loss aversion and endowment effectGains make us happy but losses make us really miserable. And once you own something, you start to value more on what you will give up and less on what you will gain by exchanging it. Example: Prepaid commission for sales students who have a mug not want to exchange it for a candy from students who have candy, same for the opposite Application: 401k matching from company 30 days money back guarantee Market and and social normsGifts build up social capital, while money reduce it Example: Volunteer job vs paid job Benefits of company Late fine for parents if the pick up their kid too late. Small fine actually make parents more likely to be late. Complete contract reduce social norm, and make people hard to agree on the sprit of contact. Solution:Think of where you are at the range of social norm to market norm and if it is suitable. Micro payments Keep price same make us easier to buy Consolidate multiple purchases reduce pain of paying Pre-paid option reduce micro-payment and reduce pain of paying Free version increate the barrier of purchase, largely Example: iTunes price are all $0.99 the mircopricing experiment DishonestyThe model of rational crime Lots of people cheat a little bit. People will weight rewards of cheating, possibility of being caught and punishment (Becker model: cost benefits analyses). However, dishonesty does not solely rely on cost benefits analyses. So we should consider fudge factor: People will consider themselves moral if they cheat only a little bit. Shrinking and expanding the fudge factorCheating is all about how we rationalize it. Some example of manipulate it:Shrink Being reminded of morality (however, remind afterward will not work). Expand Increase distant from money. Social acceptance (seeing others from same social group cheat). Creativity (Create story to defend yourself) Solution:Control small crimes, lots of small crimes may make a big difference. Conflicts of interestConflicts of interest operate within the fudge factor: People tend to rationalize towards benefits/rewards. Example: Data manipulate in science research. People like the art from gallery who give cash reward. Solution:Try to eliminate the result of interest conflict Cheating Over time and across culture People start to cheat a little and at some point, they start to cheat a lot. Confession provide a way to reset, which cut down the way to cheat more. People shared the same basic fudge factor across cultures. However, culture can shrink/expand fudge factor on a specific domain. Application:Reconciliation act for South Africa Guest speak Medical decisions: People tend to make different decision for their own than for others. (Peter Ubel) Moral pendulum: People tend to make up after doing bad / indulge after doing good. (Nina Mazar) Labor and MotivationWe are motivated to do things that we find meaningful. Sisyphus condition (Meaning)Doing same job repeatedly without meaning will be demotivate. And in this case, their internal love for this procedure will not reduce their demotivation. Acknowledgement It is relatively easy to make people feel good about their work. (Just a simple acknowledgement) In contrast, simply ignoring people can be as demotivating as destroying their work. SolutionInject more meaning to various situation. IKEA effect (Labor) Labor lead to love. Builder mistakenly think everyone will love it. Labor lead to love only when builder finish the task and create something. (Too much effort will have negative effect) Example Your own child. Self-designed T-shirt/shoes. Girls chased by boys ApplicationCustomization is more about preference, but the effort people put in. Not-invented-here biasIKEA effect on idea: people love their solution than others. SolutionWe should love what we do but now too much. Cognitive DissonanceWe behave one way but don’t believe in the same way. So we shift our belief afterwards. Example Boring task with high pay is boring while low pay is more interesting (people shift because they don’t want to admit they are doing boring job for low payment) Zappos hiring process (People convince they love the company because they give up $2000 by deciding join the company) Monetary Stress and Performance Bonus’s effect follows Yerkes-Dodson Curve except it is only for pure mechanical task. Keep getting bonus will lead to future loss aversion and continue to decrease performance. SolutionA state of “flow” drive the best performance (immersed in task). InspirationIs current high-tech company’s bonus system working? Social Stress and PerformanceHigher motivation does not necessarily result in better performance: Anxiety caused by public pressure impedes performance. ExampleNBA player have lower free throw rate in the last 5 minutes. Bonus, Labor and Motivation (Summary)Bonus:Small amount of money can move relationship from social to financial domain. Large amount can increase motivation but decrease performance. And we do not operate by simple rules of rewards but multiple combination. Motivation:Current world have more potential to find the meaning of our work. So A production line environment may not be as efficient as before. And people are less easy to be supervised. Solution Think of motivation not only by money: A thoughtful gift can mean much more than money bonus. Make work has meaning and trust employee to do the right thing. So that people will connect to the output. Guest speak (Lailn Anik) At different stage of life, motivation change. When facing different type task, bonus and pressure work differently. Self-controlDifficulty with self-controlPresent focus bias: Give more weight to the current state and environment. Example: Global warming and how to incentive people to help environment. Toyota Prius: Social rewards of hybrid driving. Solution Reward Substitution: Using an alternate reward that is intermediate and more motivating. Ulysses (Self-control) Contracts: Enforce self-control by removing the tempting. Distracted yourself from temptations. Eliminate decision point. (Religious people do not think of smoking on Sabbath) Maximize Reward Lotteries Randomization Counterfactuals Regret Reward Substitution FreedomFreedom reduce power of Ulysses Contracts. We must find a balance between the amount of freedom we crave and the control we need to shield us from tempting. Emotion Basic part of human. Can overcome cognition. Can work both for or against us. Two systemsThe limbic systems: Similar across all animals.The cognitive system: Mark us as human. Example Emotion are transient and more short-live than we expect. Emotion can over take cognition. Example: People being sexual aroused solutionUnderstand how emotion can change us. Intro-empathy problemWe making decision under a specific emotional state, we have Intro-empathy gap: Thinking we will have the same emotional state in the future Example Movie choice diffs at different emotion. The thirsty you are, the more likely you will donate to charity. The Identifiable Victim EffectA single victim inspire action while generation information about massive victims does not (statistic information inspire cognitive thinking) CauseThe discrepancy between cognitive and emotional thinking. Example The mismatch between actual need and money donated to each event/disease. The trolley problem. Donation for people in another side of the world. SolutionPortray right information (Break big problem into small pieces) to people if you want to convey them. Emotional Decision MakingFor products that are consumed emotionally, lack of cognitive input can lead to increased enjoyment. ExampleBuying jams without knowing any cognitive information (price, brand etc). SolutionThe decision environment should match up with the consumption environment. ExampleBuying ugly speaker with better sound VS good-looking speaker with worse sound. Risk AssessmentWe value risk higher when: Recently being reminded (salient in memory). We have no control. We have emotional response. Example Kill by shark (reminded by lots of movies) VS by airplane wreckage drop from sky Terrorist (lack of control) VS Car accident MindnodeConverting Markdown to a mind map LinksBook Notes of Predictably Irrational]]></content>
      <categories>
        <category>Notes</category>
        <category>Psychology</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Predictably Irrational</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CO-OP at insightFinder]]></title>
    <url>%2FWork-as-entry-level-engineer-at-start-up%2F</url>
    <content type="text"><![CDATA[After my summer internship at Facebook, I started to work at insightFinder during my second master’s year. As a star-up company, insightFinder gave me a lot of different experience than working at a large tech company such as Facebook, and taught different things. The StoryOnboardingStarting to work is quite simple here. One of my friend as well as classmate took me to our office on my first date to work. He worked here during summer and will continue for the following semester, and he will be the one to guide me. We had lunch together with two other co-workers and they introduced our overall infrastructure on the first day. Then I spent most of the day setting up my environment and making things running. Our stack includes Java backend, javascript UI, RabbitMQ, in-house C++ machine learning library and several agents on Splunk, AWS and Logstash. I was mainly focus on the backend development at that time and got a easy-fix for warm up. Struggle as a noobAt the beginning, I spent most of time learning and discussing. As I got familiar with the development cycle, I was soon assigned to improve an existing feature. This is the time I really struggle with. I know the overall structure, but I hardly understand the code base, which is to me poorly maintained and documented. I have to ask people around about how things work and where should I start with, and no one knows everything! Sometimes I have to call previous employee to get the answer and even our CEO had taught me for several times. After struggling for some days I got to know how backend is working here, and began to make progress on each tasks. Well, months later I realized the quality of work at this time is not good, code are buggy and not well formatted, and I have to rewrite some of them. But still, it feels good when making progress. Make progress and celebrateThough struggled, I was learning and finishing projects one after another. Gradually I realized there are parts of the system I was responsible for, and as a member in a small but busy team, that means you may working on things beyond coding. I started joining meetings with customers, writing user guides and things like that. It is interesting to know things like these and I feel close to the company. In the mean time the team is also making progress. We got new customers and demos came one after another. And extra hours always needed before a demo day. Life is stressful but rewarding. Good news is a new investment allows us to expand the team. We some both new-grad and senior engineers coming to reduce the burden. And a bigger office is awaiting. Dive deep with experienceAfter the first semester, I kept working on winter break. Since then I worked not only on individual feature projects but also on infrastructure design, performance improvement and reliability improve. I got the chance to resolve several data processing bottleneck which is much more challenging and interesting. Also, there are new coming engineers. And we started to enhance our code review as well as CI flow. so I started to answer questions instead of asking. I need to explain some of my work as well and assign some tasks to interns. Trying to explain and help drove me to re-think and I got deeper understanding, and make me feel I am not a noob anymore. LeavingGraduation means farewell. Last few weeks is still busy but I put some time to turn some work into a framework and hand over my things to another full time. And kept answering questions and saying goodbye by phone after leaving office. It’s my fortune to be part on this company and really hope these guys have a bright future. Some ThoughtsWhy an existing system should sucksEvery company have old, existing system and most of time we will work based on it. I was like to complain of it, as it looks big, merely maintainable, and hard to use. However, I gradually realized maybe it was good, and expandable enough when it was first developed. It only became a monster when our needs changed, or people left or simply forgot. And when we trying to replace it with a new system, the same complains started after some times because of the same reason. How to resolve this? I am not sure, maybe just kept a good development practice and a peaceful attitude when you encounter a problem. Before answer a tech questionI got a lot of questions to answer when new engineer came into team. Some days I feel like my main job is to talk and answer various questions, from infra design, code base to why a little bug happened, how to use a tool. Then I found there is something wrong with it, some questions are too simple that I don’t even need to think to give the answer, some however, I have no clue to know the answer. And sometimes, even if I have a answer, the questioner came back with another question completely different to the first one. So I start to think of how to make this communication more efficient so that I can still have time on my own work. I began to ask questioner some questions before they start their questioning: What is the task they asked you to solve? What are you working on to solve it? Do you have better plan or another idea? It is surprising efficient, most of the time they found the solution themselves, or they realized they are heading for a wrong direction. This save me the energy of doing context switch for answer a specific questions and can get back to my work easier. Prepare for meetings and reportsMeetings here are more frequent, and more roles are attending. CEO, tech lead, designer, sales, and even customer. I am not a good talker and I feel sad when my explanation is misunderstood or ignored. So make sure I kept these in mind when reporting: Make sure everyone have the same context, introduce previous context if someone is missing last time, update terms and make sure everyone is using the same. Stop taking about things others can’t understand. Not everyone knows “CPU bottleneck”, “network throughput”. Use “the user need to wait for around 10 minutes because our sever have a performance problem” instead of “our message queue is blocked because of CPU bottleneck” Kept the key point first. What problem you found, and what solution you are trying. You will lose audience when you start with introducing the root cause first. Stop meaningless talk. “It can not be done”, “It is wrong”, “It can not happened”. No one wants to hear them, talk about solution, reason instead. Start-up vs Big companyThere is much more freedom in a start-up company like this. We move faster than Facebook and it’s very exciting to see my own work delivered for a big impact. However, sometimes it is too busy and I feel a little tried. And sometimes I have work on several tasks in parallel and context switch is causing me pain and make me stressful. And there is less infrastructure to help us with the quality of work. The PicsComing soon]]></content>
      <categories>
        <category>Work experience</category>
      </categories>
      <tags>
        <tag>insightFinder</tag>
        <tag>internship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Personal Blog based on Hexo + NexT]]></title>
    <url>%2Fmy-site-development-log-hexo-nexT-GithubPage%2F</url>
    <content type="text"><![CDATA[I played with hexo to build my first personal website on 2016. Then it was untouched for a long time until today, I decided to pick things up and try to learn some UI staff with it. This page is mainly for recording what I learnt and and how I used them on this site including 3rd party plugins, theme modification and SEO. Add pluginsGitment commentsGitment is a comment system based on GitHub Issues, which can be used in the frontend without any server-side implementation. Gitment plugin is supported in current NexT(v5.1.4), just need a few steps to set up: Create OAuth application Go to settings of your github account Go to Developer settings Click Register a new application Fill in: 1234Application name：GitmentHomepage URL：&lt;your website link&gt;Application description：&lt;Anything&gt;Authorization callback URL：&lt;your website link&gt; Copy Client ID and Client Secret for future use Create repository for gitment Create a new repository named gitment-comments Change NexT config Search gitment in config.yml12345678910111213gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide &apos;Powered by ...&apos; on footer, and more language: # Force language, or auto switch by theme github_user: # MUST HAVE, Your Github ID github_repo: `gitment-comments` # MUST HAVE, The repo you use to store Gitment comments client_id: `Client ID` # MUST HAVE, Github client id for the Gitment client_secret: `Client Secret` # EITHER this or proxy_gateway, Github access secret token for the Gitment proxy_gateway: # Address of api proxy, See: https://github.com/aimingoo/intersect redirect_protocol: # Protocol of redirect_uri with force_redirect_protocol when mint enabled Add various plugins based on hexo document baidu analytics addThis share leancloud_visitors Local Search Customize pageTopX: Hottest post page (Need Leancloud support)Reference：5.8 添加 TopX 页面Basically it create a new page and rearranged post based on views from Leancloud’s database we created when we add leancloud_visitors plugin. Create new page 1hexo new page &quot;top&quot; Add menu in theme configuration 12menu: top: /top/ || signal Modify the new index.md for “top” page: 1234567891011121314151617181920212223242526272829---title: TopXcomments: falsekeywords: top,文章阅读量排行榜---&lt;div id=&quot;top&quot;&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js&quot;&gt;&lt;/script&gt;&lt;script&gt;AV.initialize(&quot;app_id&quot;, &quot;app_key&quot;);&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; var time=0 var title=&quot;&quot; var url=&quot;&quot; var query = new AV.Query(&apos;Counter&apos;); query.notEqualTo(&apos;id&apos;,0); query.descending(&apos;time&apos;); query.limit(1000); query.find().then(function (todo) &#123; for (var i=0;i&lt;1000;i++)&#123; var result=todo[i].attributes; time=result.time; title=result.title; url=result.url; var content=&quot;&lt;a href=&apos;&quot;+&quot;https://reuixiy.github.io&quot;+url+&quot;&apos;&gt;&quot;+title+&quot;&lt;/a&gt;&quot;+&quot;&lt;br /&gt;&quot;+&quot;&lt;font color=&apos;#555&apos;&gt;&quot;+&quot;阅读次数：&quot;+time+&quot;&lt;/font&gt;&quot;+&quot;&lt;br /&gt;&lt;br /&gt;&quot;; document.getElementById(&quot;top&quot;).innerHTML+=content &#125; &#125;, function (error) &#123; console.log(&quot;error&quot;); &#125;);&lt;/script&gt; Replace variables: Change app_id and app_key in index.md for your own leancloud id. Also you can change TopX’s X by changing this line: query.limit(1000); Learning sourceHexoHexo搭建博客教程Hexo+NexT 博客搭建相册打造个性超赞博客Hexo+NexT+GithubPages的超深度优化hexo的next主题个性化配置教程hexo教程 Version controlUsing Git Submodules to Manage Your Custom Hexo Theme SEOhexo高阶教程：想让你的博客被更多的人在搜索引擎中搜到吗动动手指，不限于NexT主题的Hexo优化（SEO篇Hexo Seo优化让你的博客在google搜索排名第一 CautionsConfig overrideWe should be careful with config override. If you want to override one item from a section, you should include other items of that section. For example, once I forget to declare other sections except “display” for sidebar config:12sidebar: display: always This cause the theme config is missing other sections including “offset” which is critical to sidebar affix. So I have a wrongly placed sidebar. I should change to following config if I want to use sidebar:1234567sidebar: position: left display: always offset: 12 b2t: false scrollpercent: false onmobile: false To do add timeline on about page Add Chinese post page Improve config override, it cause pain in the ass! Add SEO (Baidu + Google) Add TopX page Add photo of post and correct photo links]]></content>
      <categories>
        <category>UI</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>nextT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to do dynamic programming]]></title>
    <url>%2Fdp%2F</url>
    <content type="text"><![CDATA[The key for dp is to find the variables to represent the states and deduce the transition function. (aka find state -&gt; find relation -&gt; find constraints) Take Best Time to Buy and Sell Stock with Cooldown as an example: Find stateThe natural states for this problem is the 3 possible transactions : buy, sell, rest. Because the transaction sequences can end with any of these three states. For each of them we make an array, buy[n], sell[n] and rest[n]., which represents the maxProfit for any sequence end with buy/sell/rest. Find relationThen we want to deduce the transition functions for buy sell and rest. By definition we have: buy[i] = max(rest[i-1]-price, buy[i-1]) sell[i] = max(buy[i-1]+price, sell[i-1]) rest[i] = max(sell[i-1], buy[i-1], rest[i-1])Where price is the price of day i. All of these are very straightforward. They simply represents : We have to rest before we buy and we have to buy before we sell Find constraintsOne tricky point is how do you make sure you sell before you buy, since from the equations it seems that [buy, rest, buy] is entirely possible. Well, the answer lies within the fact that buy[i] &lt;= rest[i] which means rest[i] = max(sell[i-1], rest[i-1]). That made sure [buy, rest, buy] is never occurred. A further observation is that and rest[i] &lt;= sell[i] is also true therefore rest[i] = sell[i-1]Substitute this in to buy[i] we now have 2 functions instead of 3: buy[i] = max(sell[i-2]-price, buy[i-1]) sell[i] = max(buy[i-1]+price, sell[i-1]) This is better than 3, but, we can do even better Since states of day i relies only on i-1 and i-2 we can reduce the O(n) space to O(1). And here we are at our final solution: 1234567891011public int maxProfit(int[] prices) &#123; int sell = 0, prev_sell = 0, buy = Integer.MIN_VALUE, prev_buy; for (int price : prices) &#123; prev_buy = buy; buy = Math.max(prev_sell - price, prev_buy); prev_sell = sell; sell = Math.max(prev_buy + price, prev_sell); &#125; return sell;&#125;]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Algorithms</tag>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
</search>
